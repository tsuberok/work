{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be4bf5c-c0d8-4cd2-a3b4-ed5d62ab9c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from torch.utils.data import Dataset\n",
    "import json \n",
    "from PIL import Image\n",
    "import gc\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "import sys\n",
    "import ctypes\n",
    "import _ctypes#the hell is this?\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "cpu_device = torch.device('cpu')\n",
    "cuda_device = torch.device('cuda')\n",
    "DTYPENET = torch.float32\n",
    "DTYPEDATA = torch.float32 #maybe even float16. Sadly it doesn't do anything\n",
    "DEVICE = cpu_device\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-4\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "    #DEVICE = cuda_device\n",
    "    #torch.cuda.empty_cache()\n",
    "print(DEVICE)\n",
    "\n",
    "padding00 = 1\n",
    "ALLLAYERS = True\n",
    "\n",
    "\n",
    "MAXARRLEN = 64*(32+2*padding00)*(32+2*padding00)\n",
    "STARRLEN = 3*(32+2*padding00)*(32+2*padding00)\n",
    "INPUTFILENAME = \"inputTensorPaddedFlat.txt\"\n",
    "PADMASKFILENAME = \"paddingmask.txt\"\n",
    "OUTPUTFILENAME = \"output.txt\"\n",
    "LOGFILENAME = \"log.txt\"\n",
    "ARRFROMWHICHTOREADOUTPUT = \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90aada6b-869c-4658-b725-2f43f0169ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitConv2d(nn.Conv2d):\n",
    "    def __init__(self, *args, num_bits: int = 8, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_bits = num_bits\n",
    "\n",
    "        self.eps:float = 1e-5\n",
    "        self.quantization_range: int = 2 ** (num_bits - 1) # Q_b in the paper\n",
    "\n",
    "\n",
    "    def ste_weights(self, weights_gamma: float) -> Tensor:\n",
    "        eps: float = 1e-7\n",
    "        scaled_weights:Tensor = self.weight / (weights_gamma + eps)\n",
    "        bin_weights_no_grad: Tensor = torch.clamp(torch.round(scaled_weights), min=-1, max=1)\n",
    "        bin_weights_with_grad: Tensor = (bin_weights_no_grad - self.weight).detach() + self.weight\n",
    "        return bin_weights_with_grad\n",
    "\n",
    "\n",
    "    def binarize_weights(self, weights_gamma: float) -> Tensor:\n",
    "        binarized_weights = self.ste_weights(weights_gamma)\n",
    "        return binarized_weights\n",
    "\n",
    "\n",
    "    def quantize_activations(self, _input:Tensor, input_gamma: float) -> Tensor:\n",
    "        # Equation 4 BitNet paper\n",
    "        quantized_input = torch.clamp(\n",
    "                _input * self.quantization_range / input_gamma,\n",
    "                -self.quantization_range + self.eps,\n",
    "                self.quantization_range - self.eps,\n",
    "            )\n",
    "        return quantized_input\n",
    "\n",
    "\n",
    "    def dequantize_activations(self, _input: Tensor, input_gamma: float, beta: float) -> Tensor:\n",
    "        return _input * input_gamma * beta / self.quantization_range\n",
    "\n",
    "\n",
    "    def forward(self, _input: Tensor) -> Tensor:\n",
    "        normalized_input: Tensor = nn.functional.layer_norm(_input, (_input.shape[1:]))\n",
    "        input_gamma: float = normalized_input.abs().max().item()\n",
    "        weight_abs_mean: float = self.weight.abs().mean().item()\n",
    "\n",
    "        binarized_weights = self.binarize_weights(weight_abs_mean)\n",
    "        input_quant = self.quantize_activations(normalized_input, input_gamma)\n",
    "        output = torch.nn.functional.conv2d(\n",
    "            input=input_quant,\n",
    "            weight=binarized_weights,\n",
    "            bias=self.bias,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            dilation=self.dilation,\n",
    "            groups=self.groups\n",
    "        )#input=input_quant\n",
    "        output = self.dequantize_activations(output, input_gamma, weight_abs_mean)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class BitLinear(nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        num_bits: int = 8,\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.eps:float = 1e-5\n",
    "        self.quantization_range: int = 2 ** (num_bits - 1) # Q_b in the paper\n",
    "\n",
    "\n",
    "    def ste_weights(self, weights_gamma: float) -> Tensor:\n",
    "        eps: float = 1e-7\n",
    "        scaled_weights:Tensor = self.weight / (weights_gamma + eps)\n",
    "        bin_weights_no_grad: Tensor = torch.clamp(torch.round(scaled_weights), min=-1, max=1)\n",
    "        bin_weights_with_grad: Tensor = (bin_weights_no_grad - self.weight).detach() + self.weight\n",
    "        return bin_weights_with_grad\n",
    "\n",
    "\n",
    "    def binarize_weights(self, weights_gamma: float) -> Tensor:\n",
    "        binarized_weights = self.ste_weights(weights_gamma)\n",
    "        return binarized_weights\n",
    "\n",
    "\n",
    "    def quantize_activations(self, _input:Tensor, input_gamma: float) -> Tensor:\n",
    "        # Equation 4 BitNet paper\n",
    "        quantized_input = torch.clamp(\n",
    "                _input * self.quantization_range / input_gamma,\n",
    "                -self.quantization_range + self.eps,\n",
    "                self.quantization_range - self.eps,\n",
    "            )\n",
    "        return quantized_input\n",
    "\n",
    "\n",
    "    def dequantize_activations(self, _input: Tensor, input_gamma: float, beta: float) -> Tensor:\n",
    "        return _input * input_gamma * beta / self.quantization_range\n",
    "\n",
    "\n",
    "    def forward(self, _input: Tensor) -> Tensor:\n",
    "        normalized_input: Tensor =nn.functional.layer_norm(_input,(_input.shape[1:]))\n",
    "        input_gamma: float = normalized_input.abs().max().item()\n",
    "        weight_abs_mean: float = self.weight.abs().mean().item()\n",
    "\n",
    "        binarized_weights = self.binarize_weights(weight_abs_mean)\n",
    "        input_quant = self.quantize_activations(normalized_input, input_gamma)\n",
    "        output = torch.nn.functional.linear(input_quant, binarized_weights, self.bias)#input_quant\n",
    "        output = self.dequantize_activations(output, input_gamma, weight_abs_mean)\n",
    "\n",
    "        return output\n",
    "\n",
    "class CNNBLOCK_DS(nn.Module):#i should use more *ags and **kwargs\n",
    "    def __init__(self, in_channels_, out_channels_,\n",
    "                 kernel_size_=3, stride_=1, \n",
    "                 padding_=1, bias_=False):\n",
    "        super().__init__()\n",
    "        self.conv_depth = BitConv2d(in_channels = in_channels_, \n",
    "                                    out_channels =in_channels_, \n",
    "                                    kernel_size =kernel_size_, \n",
    "                                    stride = stride_, \n",
    "                                    padding = padding_, \n",
    "                                    dilation = 1,\n",
    "                                    groups =in_channels_,\n",
    "                                    bias = bias_)\n",
    "        self.conv_sep = BitConv2d(in_channels = in_channels_, \n",
    "                                  out_channels = out_channels_, \n",
    "                                  kernel_size=1, \n",
    "                                  stride = 1,\n",
    "                                  padding= 0, \n",
    "                                  dilation=1,\n",
    "                                  groups = 1,\n",
    "                                  bias=False)\n",
    "        self.lrlu = nn.LeakyReLU(0.1)\n",
    "    def forward(self, x):\n",
    "        return self.lrlu(self.conv_sep(self.conv_depth(x)))#i think we need bn here\n",
    "        \n",
    "\n",
    "class RESBLOCK(nn.Module):\n",
    "    def __init__(self, list_of_params):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        for block in list_of_params:\n",
    "            in_channels_ = block[0]\n",
    "            out_channels_ = block[1]\n",
    "            modules.append(CNNBLOCK_DS(in_channels_, out_channels_))\n",
    "        self.suka = nn.Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return x + self.suka(x)\n",
    "\n",
    "class YOLONET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.convBlock1 = CNNBLOCK_DS(3, 64)\n",
    "        \n",
    "        #we need 5 32 layers, 5 64 layers, 5 128 layers and 5 256 layers. Don't forget to maxpool\n",
    "        self.resblock64_1 = RESBLOCK([[64, 64], [64,64] ])#how many layers should i have in my resblock???\n",
    "        self.resblock64_2 = RESBLOCK([[64, 64], [64,64] ])\n",
    "        self.resblock64_3 = RESBLOCK([[64, 64], [64,64] ])\n",
    "        self.resblock64_4 = RESBLOCK([[64, 64], [64,64] ])\n",
    "        self.resblock64_5 = RESBLOCK([[64, 64], [64,64] ])\n",
    "\n",
    "        #how do we upscale??\n",
    "        self.convInter_64_128 = CNNBLOCK_DS(64, 128)\n",
    "        self.resblock128_1 = RESBLOCK([[128, 128], [128, 128]])\n",
    "        self.resblock128_2 = RESBLOCK([[128, 128], [128, 128]])\n",
    "        self.resblock128_3 = RESBLOCK([[128, 128], [128, 128]])\n",
    "        self.resblock128_4 = RESBLOCK([[128, 128], [128, 128]])\n",
    "        self.resblock128_5 = RESBLOCK([[128, 128], [128, 128]])\n",
    "        self.convInter_128_256 = CNNBLOCK_DS(128, 256)\n",
    "        self.resblock256_1 = RESBLOCK([[256, 256], [256, 256]])\n",
    "        self.resblock256_2 = RESBLOCK([[256, 256], [256, 256]])\n",
    "        self.resblock256_3 = RESBLOCK([[256, 256], [256, 256]])\n",
    "        self.resblock256_4 = RESBLOCK([[256, 256], [256, 256]])\n",
    "        self.resblock256_5 = RESBLOCK([[256, 256], [256, 256]])\n",
    "        self.resblock256_6 = RESBLOCK([[256, 256], [256, 256]])\n",
    "        \n",
    "        #self.convLast = nn.Conv2d(128, 16, 1 padding=0, bias=False)#how do we ensure we have [:, 16, 13, 13] output tensor????\n",
    "        self.avgPool = nn.AvgPool2d(2,3)\n",
    "        self.fc = BitLinear(256, 10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.convBlock1(x)\n",
    "        x = self.resblock64_1(x)\n",
    "        x = self.resblock64_2(x)\n",
    "        x = self.resblock64_3(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.resblock64_4(x)\n",
    "        x = self.resblock64_5(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.convInter_64_128(x)\n",
    "        x = self.resblock128_1(x)\n",
    "        x = self.resblock128_2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.resblock128_3(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.resblock128_4(x)\n",
    "        x = self.resblock128_5(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.convInter_128_256(x)\n",
    "        x = self.resblock256_1(x)\n",
    "        x = self.resblock256_2(x)\n",
    "        x = self.resblock256_3(x)\n",
    "        x = self.resblock256_4(x)\n",
    "        x = self.resblock256_5(x)\n",
    "        x = self.resblock256_6(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d1094d-cade-4736-b756-68c0228af501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUKAAAA torch.Size([10])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "getLayerDimNet = YOLONET()\n",
    "toSeeLayerDim = torch.squeeze(getLayerDimNet(torch.rand((1,3,32,32))) , 0)\n",
    "print(\"SUKAAAA\", toSeeLayerDim.shape)\n",
    "padded00 = toSeeLayerDim.clone()\n",
    "if ALLLAYERS == False:\n",
    "    padded00 = torch.zeros((toSeeLayerDim.shape[0], toSeeLayerDim.shape[1]+2*padding00, toSeeLayerDim.shape[2]+2*padding00))\n",
    "    padded00[:,padding00:padding00+toSeeLayerDim.shape[1],padding00:padding00+toSeeLayerDim.shape[2]] = toSeeLayerDim\n",
    "OUTPUTARRAYLEN = len(padded00.flatten())\n",
    "print(OUTPUTARRAYLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09c349b-6577-4413-b908-b78810eed893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitConv2dBitProba(nn.Conv2d):\n",
    "    def __init__(self,weightAbsMean: float, *args,num_bits: int = 8, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weightAbsMean = weightAbsMean\n",
    "        self.num_bits = num_bits\n",
    "        self.eps:float = 1e-5\n",
    "        self.quantization_range: int = 2 ** (num_bits - 1) # Q_b in the paper\n",
    "    def quantize_activations(self, _input:Tensor, input_gamma: float) -> Tensor:\n",
    "        # Equation 4 BitNet paper\n",
    "        quantized_input = torch.clamp(\n",
    "                _input * self.quantization_range / input_gamma,\n",
    "                -self.quantization_range + self.eps,\n",
    "                self.quantization_range - self.eps,)\n",
    "        return quantized_input\n",
    "    def dequantize_activations(self, _input: Tensor, input_gamma: float, beta: float) -> Tensor:\n",
    "        return _input * input_gamma * beta / self.quantization_range\n",
    "    def forward(self, _input: Tensor) -> Tensor:\n",
    "        normalized_input: Tensor = nn.functional.layer_norm(_input, (_input.shape))\n",
    "        input_gamma: float = normalized_input.abs().max().item()\n",
    "        weight_abs_mean: float = self.weight.abs().mean().item()#i guess here is the crux of the problem\n",
    "        input_quant = self.quantize_activations(normalized_input, input_gamma)\n",
    "        output = torch.nn.functional.conv2d(\n",
    "            input=input_quant,\n",
    "            weight=self.weight,\n",
    "            bias=self.bias,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            dilation=self.dilation,\n",
    "            groups=self.groups)\n",
    "        output = self.dequantize_activations(output, input_gamma, self.weightAbsMean)\n",
    "        return output\n",
    "\n",
    "class BitLinearBitProba(nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weightAbsMean: float,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        num_bits: int = 8,\n",
    "    ):#i've no idea if the original network has bias in here\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.eps:float = 1e-5\n",
    "        self.weightAbsMean = weightAbsMean\n",
    "        self.quantization_range: int = 2 ** (num_bits - 1) # Q_b in the paper\n",
    "    def quantize_activations(self, _input:Tensor, input_gamma: float) -> Tensor:\n",
    "        # Equation 4 BitNet paper\n",
    "        quantized_input = torch.clamp(\n",
    "                _input * self.quantization_range / input_gamma,\n",
    "                -self.quantization_range + self.eps,\n",
    "                self.quantization_range - self.eps,)\n",
    "        return quantized_input\n",
    "    def dequantize_activations(self, _input: Tensor, input_gamma: float, beta: float) -> Tensor:\n",
    "        return _input * input_gamma * beta / self.quantization_range\n",
    "    def forward(self, _input: Tensor) -> Tensor:\n",
    "        normalized_input: Tensor = nn.functional.layer_norm(_input, (_input.shape))\n",
    "        input_gamma: float = normalized_input.abs().max().item()\n",
    "        weight_abs_mean: float = self.weight.abs().mean().item()\n",
    "        input_quant = self.quantize_activations(normalized_input, input_gamma)\n",
    "        output = torch.nn.functional.linear(input_quant, self.weight, self.bias)\n",
    "        output = self.dequantize_activations(output, input_gamma, self.weightAbsMean)   \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e456fb0a-68eb-49ba-ae5d-db3b2cf37944",
   "metadata": {},
   "outputs": [],
   "source": [
    "zah0 = f'''\n",
    "#include <stdio.h>\n",
    "#include <stdbool.h>\n",
    "#include <math.h>\n",
    "#include <time.h>\n",
    "#include <immintrin.h>\n",
    "#include <pthread.h>\n",
    "#include <string.h>\n",
    "#define MAXARRLEN {MAXARRLEN}\n",
    "#define STARRLEN {STARRLEN}\n",
    "#define OUTPUTARRAYLEN {OUTPUTARRAYLEN}\n",
    "#define INPUTFILENAME \"{INPUTFILENAME}\"\n",
    "#define PADMASKFILENAME \"{PADMASKFILENAME}\"\n",
    "#define OUTPUTFILENAME \"{OUTPUTFILENAME}\"\n",
    "#define LOGFILENAME \"{LOGFILENAME}\"\n",
    "bool P[MAXARRLEN];\n",
    "float A[MAXARRLEN],B[MAXARRLEN],C[MAXARRLEN];\n",
    "float absmax;\n",
    "long int i;\n",
    "clock_t st, end;\n",
    "double time_spent_in_conv = 0.0;\n",
    "\n",
    "/*\n",
    "INSERT YOUR BIAS DECLARATION CODE HERE\n",
    "*/\n",
    "\n",
    "\n",
    "\n",
    "float min(float a,float b){{\n",
    "  if (a <= b){{return a;}}\n",
    "  else{{return b;}}\n",
    "}}\n",
    "float max(float a, float b){{\n",
    "  if (a >= b){{return a;}}\n",
    "  else{{return b;}}\n",
    "}}\n",
    "long int iib(int d,int i,int j,int H,int W){{return d*(H*W)+i*W+j;}}\n",
    "\n",
    "\n",
    "void padNext(float NEXTARRAY[],bool PADDINGMASK[],int padding,int D,int H,int W){{\n",
    "    long int car = 0;\n",
    "    for (int d = 0; d < D; d++){{\n",
    "        memset(&NEXTARRAY[car], 0, W*padding*sizeof(float));\n",
    "        memset(&PADDINGMASK[car], 0, W*padding*sizeof(bool));\n",
    "        car += W*padding;\n",
    "        for (int h = padding; h < H - padding; h++){{\n",
    "            memset(&NEXTARRAY[car], 0, padding*sizeof(float));\n",
    "            memset(&PADDINGMASK[car], 0, padding*sizeof(bool));\n",
    "            car += padding;\n",
    "            car += W - 2*padding;\n",
    "            memset(&NEXTARRAY[car], 0, padding*sizeof(float));\n",
    "            memset(&PADDINGMASK[car], 0, padding*sizeof(bool));\n",
    "            car += padding;\n",
    "        }}\n",
    "        memset(&NEXTARRAY[car], 0, W*padding*sizeof(float));\n",
    "        memset(&PADDINGMASK[car], 0, W*padding*sizeof(bool));\n",
    "        car += W*padding;\n",
    "    }}\n",
    "}}\n",
    "\n",
    "float layernormQuantizationBiasNextpadding(int inputD, int inputH, int inputW,\n",
    "                                           int out_channels,int stride,\n",
    "                                           float INPUTARRAY[],bool PADDINGMASK[],\n",
    "                                           float BIAS[],float OUTPUTARRAY[],\n",
    "                                           int output_padding,bool bias_flag,\n",
    "                                           int kernelSize,\n",
    "                                           int num_bits,bool linLayer_flag){{\n",
    "    long int inputArrayLen = inputD*inputH*inputW;\n",
    "    \n",
    "    long int numNonZero = 0;\n",
    "    __m256 acc0 = {{0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0}};\n",
    "    for (long int i = 0; i < 8*(inputArrayLen/8); i+=8){{\n",
    "        __m256 cur = {{PADDINGMASK[i],PADDINGMASK[i+1],PADDINGMASK[i+2],PADDINGMASK[i+3],PADDINGMASK[i+4],PADDINGMASK[i+5],PADDINGMASK[i+6],PADDINGMASK[i+7]}};\n",
    "        acc0 = _mm256_add_ps(acc0,cur);\n",
    "    }}\n",
    "    float* ptrToAcc0 = (float*)&acc0;\n",
    "    for (int i = 0; i < 8; i++){{\n",
    "        numNonZero += *ptrToAcc0;\n",
    "        ptrToAcc0 += 1;\n",
    "    }}\n",
    "    for (long int i = 8*(inputArrayLen/8); i < inputArrayLen; i++){{\n",
    "        numNonZero += PADDINGMASK[i];\n",
    "    }}\n",
    "    float mean_ = 0;\n",
    "    __m256 acc = {{0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0}};\n",
    "    for (long int i = 0; i < 8*(inputArrayLen/8); i+=8){{\n",
    "        __m256 cur = {{INPUTARRAY[i],INPUTARRAY[i+1],INPUTARRAY[i+2],INPUTARRAY[i+3],INPUTARRAY[i+4],INPUTARRAY[i+5],INPUTARRAY[i+6],INPUTARRAY[i+7]}};\n",
    "        acc = _mm256_add_ps(acc,cur);\n",
    "    }}\n",
    "    float* ptrToAcc = (float*)&acc;\n",
    "    for (int i = 0; i < 8; i++){{\n",
    "        mean_ += *ptrToAcc;\n",
    "        ptrToAcc += 1;\n",
    "    }}\n",
    "    for (long int i = 8*(inputArrayLen/8); i < inputArrayLen; i++){{\n",
    "        mean_ += INPUTARRAY[i];\n",
    "    }}\n",
    "    mean_ = mean_/(numNonZero+1e-5);\n",
    "    float var_ = 0;\n",
    "\n",
    "    __m256 acc1 = {{0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0}};\n",
    "    __m256 minus_means = {{-mean_, -mean_, -mean_, -mean_, -mean_, -mean_, -mean_, -mean_}};\n",
    "    for (long int i = 0; i < 8*(inputArrayLen/8);i+=8){{\n",
    "        __m256 cur = {{INPUTARRAY[i],INPUTARRAY[i+1],INPUTARRAY[i+2],INPUTARRAY[i+3],INPUTARRAY[i+4],INPUTARRAY[i+5],INPUTARRAY[i+6],INPUTARRAY[i+7]}};\n",
    "        cur = _mm256_add_ps(cur, minus_means);\n",
    "        cur = _mm256_mul_ps(cur,cur);\n",
    "        __m256 p_mask = {{PADDINGMASK[i],PADDINGMASK[i+1],PADDINGMASK[i+2],PADDINGMASK[i+3],PADDINGMASK[i+4],PADDINGMASK[i+5],PADDINGMASK[i+6],PADDINGMASK[i+7]}};\n",
    "        cur = _mm256_mul_ps(cur, p_mask);\n",
    "        acc1 = _mm256_add_ps(acc1, cur);\n",
    "    }}\n",
    "    float* ptrToAcc1 = (float*)&acc1;\n",
    "    for (int i = 0; i < 8; i++){{\n",
    "        var_ += *ptrToAcc1;\n",
    "        ptrToAcc1 += 1;\n",
    "    }}\n",
    "    for (long int i = 8*(inputArrayLen/8); i < inputArrayLen; i++){{\n",
    "        var_ += PADDINGMASK[i] * pow((INPUTARRAY[i]-mean_),2);\n",
    "    }}\n",
    "\n",
    "          \n",
    "    var_ = var_/(numNonZero-1);\n",
    "    float denom = sqrt(var_+1e-5);\n",
    "\n",
    "    __m256 inv_denoms = {{1/denom, 1/denom, 1/denom, 1/denom, 1/denom, 1/denom, 1/denom, 1/denom}};\n",
    "    for (long int i = 0; i < 8*(inputArrayLen/8); i+=8){{\n",
    "        __m256 cur = {{INPUTARRAY[i],INPUTARRAY[i+1],INPUTARRAY[i+2],INPUTARRAY[i+3],INPUTARRAY[i+4],INPUTARRAY[i+5],INPUTARRAY[i+6],INPUTARRAY[i+7]}};\n",
    "        cur = _mm256_add_ps(cur, minus_means);\n",
    "        cur = _mm256_mul_ps(cur, inv_denoms);\n",
    "        __m256 p_mask = {{PADDINGMASK[i],PADDINGMASK[i+1],PADDINGMASK[i+2],PADDINGMASK[i+3],PADDINGMASK[i+4],PADDINGMASK[i+5],PADDINGMASK[i+6],PADDINGMASK[i+7]}};\n",
    "        cur = _mm256_mul_ps(cur, p_mask);\n",
    "        _mm256_store_ps(&INPUTARRAY[i], cur);\n",
    "        //INPUTARRAY[i] = PADDINGMASK[i] * (INPUTARRAY[i]-mean_)/denom;\n",
    "    }}\n",
    "    \n",
    "    for (long int i = 8*(inputArrayLen/8); i < inputArrayLen; i++){{\n",
    "        INPUTARRAY[i] = PADDINGMASK[i] * (INPUTARRAY[i]-mean_)/denom;\n",
    "    }}\n",
    "    \n",
    "    float absmax = fabsf(INPUTARRAY[0]);\n",
    "    for (long int i = 1; i < inputArrayLen; i++){{fabsf(INPUTARRAY[i]) > absmax ? absmax = fabsf(INPUTARRAY[i]) : 0;}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    float quantizationRange = pow(2,(num_bits-1));\n",
    "    float minclamp  = -quantizationRange+1e-5;\n",
    "    float maxclamp = +quantizationRange+1e-5;\n",
    "    float factor_quant = quantizationRange/absmax;\n",
    "    __m256 factor_quant_vec = {{factor_quant,factor_quant,factor_quant,factor_quant,factor_quant,factor_quant,factor_quant,factor_quant}};\n",
    "    __m256 minclamp_vec = {{minclamp,minclamp,minclamp,minclamp,minclamp,minclamp,minclamp,minclamp}};\n",
    "    __m256 maxclamp_vec = {{maxclamp,maxclamp,maxclamp,maxclamp,maxclamp,maxclamp,maxclamp,maxclamp}};\n",
    "    for (long int i = 0; i < 8*(inputArrayLen/8); i+=8){{\n",
    "        __m256 cur = {{INPUTARRAY[i],INPUTARRAY[i+1],INPUTARRAY[i+2],INPUTARRAY[i+3],INPUTARRAY[i+4],INPUTARRAY[i+5],INPUTARRAY[i+6],INPUTARRAY[i+7]}};\n",
    "        cur = _mm256_mul_ps(cur, factor_quant_vec);\n",
    "        cur = _mm256_max_ps(cur, minclamp_vec);\n",
    "        cur = _mm256_min_ps(cur, maxclamp_vec);\n",
    "        _mm256_store_ps(&INPUTARRAY[i], cur);\n",
    "        //cur = min(max(cur,minclamp) , maxclamp);\n",
    "    }}\n",
    "    \n",
    "    for (long int i = 8*(inputArrayLen/8); i < inputArrayLen; i++){{\n",
    "        INPUTARRAY[i] = min(max(INPUTARRAY[i],minclamp), maxclamp);\n",
    "    }}\n",
    "    \n",
    "    int outputD = out_channels;\n",
    "    int outputH = (inputH-kernelSize)/stride+1+2*output_padding; \n",
    "    int outputW = (inputW-kernelSize)/stride+1+2*output_padding;\n",
    "    if (linLayer_flag){{\n",
    "        outputH = 1;\n",
    "        outputW = 1;\n",
    "    }}\n",
    "    if (bias_flag){{\n",
    "        for (int d = 0; d < outputD; d++){{\n",
    "            for (int i = output_padding; i < outputH-output_padding; i++){{\n",
    "                for (int j = output_padding; j < outputW-output_padding; j++){{\n",
    "                    OUTPUTARRAY[iib(d,i,j,outputH,outputW)] = BIAS[d];\n",
    "                    PADDINGMASK[iib(d,i,j,outputH,outputW)] = 1;}}\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    if (!bias_flag){{\n",
    "        for (int d = 0; d < outputD; d++){{\n",
    "            for (int i = output_padding; i < outputH-output_padding; i++){{\n",
    "                for (int j = output_padding; j < outputW-output_padding; j++){{\n",
    "                    OUTPUTARRAY[iib(d,i,j,outputH,outputW)] = 0;\n",
    "                    PADDINGMASK[iib(d,i,j,outputH,outputW)] = 1;}}\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    if (output_padding > 0){{\n",
    "        padNext(OUTPUTARRAY,PADDINGMASK,output_padding,outputD,outputH,outputW);}}\n",
    "    return absmax;\n",
    "}}\n",
    "\n",
    "void dequantRelu(float ARRAY[],bool PADDINGMASK[],\n",
    "                 long int arrayLen,float absmax,\n",
    "                 float absmeanweight,int num_bits,\n",
    "                 float neg_slope,bool non_linearity){{\n",
    "    float quantizationRange = pow(2,(num_bits - 1));\n",
    "    float factor = absmax*absmeanweight/quantizationRange;\n",
    "    __m256 factor_vec = {{factor, factor, factor, factor, factor, factor, factor, factor}};\n",
    "    for (long int i = 0; i < 8*(arrayLen/8); i+=8){{\n",
    "        __m256 cur = {{ARRAY[i],ARRAY[i+1],ARRAY[i+2],ARRAY[i+3],ARRAY[i+4],ARRAY[i+5],ARRAY[i+6],ARRAY[i+7]}};\n",
    "        cur = _mm256_mul_ps(cur, factor_vec);\n",
    "        _mm256_store_ps(&ARRAY[i], cur);\n",
    "    \t//PADDINGMASK[i] ? ARRAY[i] = ARRAY[i]*factor : 0;\n",
    "    }}\n",
    "    \n",
    "    for (long int i = 8*(arrayLen/8); i < arrayLen; i++){{\n",
    "        ARRAY[i] = ARRAY[i]*factor;\n",
    "        //PADDINGMASK[i] ? ARRAY[i] = ARRAY[i]*factor : 0;\n",
    "    }}\n",
    "    \n",
    "    if (non_linearity){{\n",
    "        __m256 neg_slope_vec = {{neg_slope, neg_slope, neg_slope, neg_slope, neg_slope, neg_slope, neg_slope, neg_slope}};\n",
    "        __m256 zero_vec = {{0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0}};\n",
    "        for (long int i = 0; i < 8*(arrayLen/8); i+=8){{\n",
    "            __m256 cur = {{ARRAY[i],ARRAY[i+1],ARRAY[i+2],ARRAY[i+3],ARRAY[i+4],ARRAY[i+5],ARRAY[i+6],ARRAY[i+7]}};\n",
    "            __m256 minarr = _mm256_min_ps(zero_vec, cur);\n",
    "            minarr = _mm256_mul_ps(minarr, neg_slope_vec);\n",
    "            __m256 maxarr = _mm256_max_ps(zero_vec, cur);\n",
    "            __m256 rest = _mm256_add_ps(minarr, maxarr);\n",
    "            _mm256_store_ps(&ARRAY[i], rest);\n",
    "    \t    //PADDINGMASK[i] ? ARRAY[i] = max(0,ARRAY[i])+neg_slope*min(0,ARRAY[i]) : 0;\n",
    "        }}\n",
    "        \n",
    "        for (long int i = 8*(arrayLen/8); i < arrayLen; i++){{\n",
    "            ARRAY[i] = max(0,ARRAY[i])+neg_slope*min(0,ARRAY[i]);\n",
    "        }}\n",
    "        \n",
    "    }}               \n",
    "}}\n",
    "\n",
    "\n",
    "void maxpool2d(int input_padding,float INPUTARRAY[],\n",
    "               int D,int inputH,int inputW,\n",
    "               int kernel_size,int stride,\n",
    "               int output_padding,float OUTPUTARRAY[],\n",
    "               bool PADDINGMASK[]){{\n",
    "    int outputD = D;\n",
    "    int outputHnp = (inputH-2*input_padding-kernel_size)/stride+1;//I HOPE THIS DIVISION WON'T CAUSE PROBLEMS\n",
    "    int outputWnp = (inputW-2*input_padding-kernel_size)/stride+1;\n",
    "    int outputH = outputHnp+2*output_padding;\n",
    "    int outputW = outputWnp+2*output_padding;\n",
    "    for (long int i = 0; i < outputD*outputH*outputW; i++){{\n",
    "        PADDINGMASK[i] = 1;}}\n",
    "    if (output_padding > 0){{\n",
    "        padNext(OUTPUTARRAY,PADDINGMASK,output_padding,outputD,outputH,outputW);}}\n",
    "    int iin = input_padding;\n",
    "    int jin = input_padding;\n",
    "    for (int d = 0; d < outputD; d++){{\n",
    "        for (int iout = output_padding; iout < outputH-output_padding; iout++){{\n",
    "            for (int jout = output_padding; jout < outputW-output_padding; jout++){{\n",
    "                float maxim = INPUTARRAY[iib(d,iin,jin,inputH,inputW)];\n",
    "                for (int rowiter = iin; rowiter < iin+kernel_size; rowiter++){{\n",
    "                    for (int coliter = jin; coliter < jin+kernel_size; coliter++){{\n",
    "                        if (INPUTARRAY[iib(d,rowiter,coliter,inputH,inputW)] > maxim){{\n",
    "                            maxim = INPUTARRAY[iib(d,rowiter,coliter,inputH,inputW)];}}\n",
    "                    }}\n",
    "                }}\n",
    "                OUTPUTARRAY[iib(d,iout,jout,outputH,outputW)] = maxim;\n",
    "                jin += stride;\n",
    "            }}\n",
    "            iin += stride;\n",
    "            jin = input_padding;\n",
    "        }}\n",
    "        iin = input_padding;\n",
    "    }}\n",
    "}}\n",
    "\n",
    "void copyWithNoPad(float INPUTARRAY[],float OUTPUTARRAY[],bool PADDINGMASK[],int D,int H,int W,int pad){{\n",
    "    long int origArrayLen = D*H*W;\n",
    "    long int newArrayLen = D*(H-2*pad)*(W-2*pad);\n",
    "    long int j = 0;\n",
    "    for (long int i = 0; i < origArrayLen; i++){{\n",
    "        if (PADDINGMASK[i] == 1){{\n",
    "            OUTPUTARRAY[j] = INPUTARRAY[i];\n",
    "            j+=1;}}\n",
    "    }}\n",
    "    for (long int j = 0; j < newArrayLen; j++){{\n",
    "        PADDINGMASK[j] = 1;}}\n",
    "}}\n",
    "\n",
    "/*\n",
    "INSERT YOUR FUNCTION DEFINITIONS HERE\n",
    "*/\n",
    "\n",
    "\n",
    "int main() {{\n",
    "/*\n",
    "INSERT YOU BIAS INIT HERE\n",
    "*/\n",
    "\n",
    "  FILE *fileArr,*fileMask;\n",
    "  fileArr = fopen(INPUTFILENAME,\"r\");\n",
    "  fileMask = fopen(PADMASKFILENAME,\"r\");\n",
    "  for (int k = 0; k < STARRLEN; k++){{\n",
    "    fscanf(fileArr,\"%f\", &A[k]);\n",
    "    fscanf(fileMask,\"%d\", &P[k]);\n",
    "  }}\n",
    "  fclose(fileArr);\n",
    "  fclose(fileMask);\n",
    "  clock_t begin = clock();\n",
    "\n",
    "/*\n",
    "INSERT YOUR FUNCTION CALLS HERE\n",
    "*/\n",
    "  \n",
    "  clock_t end = clock();\n",
    "  double total_time = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "  time_spent_in_conv = time_spent_in_conv/CLOCKS_PER_SEC;\n",
    "  FILE *fileLog = fopen(LOGFILENAME, \"w\");\n",
    "  fprintf(fileLog,\"CPU time spent: %f\\\\n\",total_time);\n",
    "  fprintf(fileLog,\"Time spent in conv: %f\\\\n\",time_spent_in_conv);\n",
    "  fprintf(fileLog,\"Time spent in conv: %f %% \\\\n\",time_spent_in_conv/total_time*100);\n",
    "\n",
    "  FILE *fileRes = fopen(OUTPUTFILENAME,\"w\");\n",
    "  for (int k = 0; k < OUTPUTARRAYLEN; k++){{\n",
    "    fprintf(fileRes,\"%f\\\\n\",{ARRFROMWHICHTOREADOUTPUT}[k]);\n",
    "  }}\n",
    "  fclose(fileRes);\n",
    "  fclose(fileLog);\n",
    "  //scanf(\"%d\");\n",
    "  return 0;\n",
    "}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e88b2d4-fc93-402b-9074-677e8e9f0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(zah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b198a1-707f-4f04-9478-a8939e9088a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def genBiasStructDeclaration(BIASES):\n",
    "    ret = \"struct bias_struct {\\n\"\n",
    "    for i in range(0,len(BIASES)):\n",
    "        if BIASES[i] == 0:\n",
    "            ret += f\"float b{i}[1];\"\n",
    "        else:\n",
    "            thisLen = len(BIASES[i])\n",
    "            ret += f\"float b{i}[{thisLen}];\"\n",
    "    ret += \"};\\n\"\n",
    "    ret += \"struct bias_struct BIASES;\\n\"\n",
    "    return ret\n",
    "\n",
    "def genPopulateBiasStruct(BIASES):\n",
    "    ret = \"\"\n",
    "    for i in range(0, len(BIASES)):\n",
    "        if BIASES[i] == 0:\n",
    "            ret += f\"BIASES.b{i}[0] = 1.0;\"\n",
    "        else:\n",
    "            for j in range(0, len(BIASES[i])):\n",
    "                ret += f\"BIASES.b{i}[{j}] = {BIASES[i][j]};\"\n",
    "    return ret + '\\n'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def genOneConvFilterNOIIB(weight,dinin,dinout,inputH,inputW,INPUTARRAYNAME,OUTPUTARRAYNAME,outputH,outputW):\n",
    "    kernel_depth = weight.shape[0]\n",
    "    kernel_size = weight.shape[1]\n",
    "    output = f'{OUTPUTARRAYNAME}[iib({dinout},iout,jout,{outputH},{outputW})]+='\n",
    "    nonzerow = 0\n",
    "    for dr in range(0,kernel_depth):\n",
    "        for ir in range(0, kernel_size):\n",
    "            for jr in range(0, kernel_size):\n",
    "                if weight[dr,ir,jr] == 1:\n",
    "                    output+=f'+{INPUTARRAYNAME}[fs+{dinin*inputH*inputW+dr*inputH*inputW+ir*inputW+jr}]'\n",
    "                    nonzerow += 1\n",
    "                if weight[dr,ir,jr] == -1:\n",
    "                    output+=f'-{INPUTARRAYNAME}[fs+{dinin*inputH*inputW+dr*inputH*inputW+ir*inputW+jr}]'\n",
    "                    nonzerow += 1\n",
    "    output+='+0;\\n'\n",
    "    path_to_add_count = \"conv_count.txt\"\n",
    "    if not os.path.exists(path_to_add_count):\n",
    "        with open(path_to_add_count, 'w') as f:\n",
    "            f.write('')\n",
    "    else:\n",
    "        with open(path_to_add_count, 'a') as f:\n",
    "            f.write(str(nonzerow)+'\\n')\n",
    "    #print(\"NON ZERO WEIGHTS\", nonzerow)\n",
    "    return output\n",
    "\n",
    "def genBitConvBitPythonNOIIB(weight,bias_flag,BIASARRAYNAME,\n",
    "                             in_channels,out_channels,\n",
    "                             kernel_size,stride,\n",
    "                             dilation,groups,\n",
    "                             INPUTARRAYNAME,\n",
    "                             OUTPUTARRAYNAME, \n",
    "                             inputH,inputW,\n",
    "                             output_padding,\n",
    "                             PADDINGMASKNAME,\n",
    "                             absmeanweight,\n",
    "                             neg_slope,num_bits,\n",
    "                             non_linearity):\n",
    "\n",
    "    outputH = (inputH-kernel_size)//stride+1+2*output_padding \n",
    "    outputW = (inputW-kernel_size)//stride+1+2*output_padding\n",
    "    #how will we call our funcntions????\n",
    "    with open(\"curFNameNo.txt\",\"r\") as f:\n",
    "        num = int(f.read())\n",
    "    with open(\"curFNameNo.txt\",\"w\") as f:\n",
    "        f.write(str(num+1))\n",
    "    with open(\"layerNames.txt\", \"r\") as f:\n",
    "        allNames = f.read().split('\\n')\n",
    "        funcName = allNames[num]\n",
    "    #what will be the function's interface?\n",
    "    #layerName(I,O,ioutStart,ioutEndPlusOne,istart,jstart,stride)   \n",
    "    fDefCode =f'''\n",
    "void {funcName}(float I[],float O[],int ioutStart,int ioutEndPlusOne,int istart,int jstart){{\n",
    "int i = istart;\n",
    "int j = jstart;\n",
    "'''\n",
    "    fDefCode += f'for (int iout = ioutStart; iout < ioutEndPlusOne; iout++){{\\n'\n",
    "    fDefCode+=f'\\tfor (int jout = {output_padding}; jout < {outputW-output_padding}; jout++){{\\n\\t\\t'\n",
    "    fDefCode+=f'long int fs = j+{inputW}*i;\\n'\n",
    "    filterDepth = in_channels//groups\n",
    "    filtersPerGroup = out_channels//groups\n",
    "    dinin = 0\n",
    "    for dinout in range(0, out_channels):\n",
    "        if dinout > 0 and dinout%filtersPerGroup==0:\n",
    "            dinin += filterDepth\n",
    "        fDefCode+= genOneConvFilterNOIIB(weight[dinout],\n",
    "                                           dinin,dinout,\n",
    "                                           inputH,inputW,\n",
    "                                           'I',\n",
    "                                           'O',\n",
    "                                           outputH,outputW)\n",
    "    fDefCode += f\"\\n\\t\\tj+={stride};}}\\n\\ti+={stride};\\n\\tj=0;}}\\n}}\\n\"\n",
    "\n",
    "    fCallCode = f'''\n",
    "absmax = layernormQuantizationBiasNextpadding({in_channels},{inputH},{inputW},{out_channels},{stride},{INPUTARRAYNAME},{PADDINGMASKNAME},{BIASARRAYNAME},{OUTPUTARRAYNAME},{output_padding},{bias_flag},{kernel_size},{num_bits},false);\n",
    "st = clock();\n",
    "{funcName}({INPUTARRAYNAME},{OUTPUTARRAYNAME},{output_padding},{outputH-output_padding},0,0);\n",
    "end = clock();\n",
    "time_spent_in_conv += (double)(end - st);\n",
    "dequantRelu({OUTPUTARRAYNAME},{PADDINGMASKNAME},{out_channels*outputH*outputW},absmax,{absmeanweight},{num_bits},{neg_slope},{non_linearity});\n",
    "'''\n",
    "    return {'fDef': fDefCode, 'fCall': fCallCode}\n",
    "\n",
    "\n",
    "def genPoolCodePython(input_padding,maxOrAvg,prevfDefs,prevfCalls,\n",
    "                      INPUTARRAY,D,\n",
    "                      inputH,inputW,\n",
    "                      kernel_size,stride,\n",
    "                      output_padding,\n",
    "                      OUTPUTARRAY,PADDINGMASK):\n",
    "    outputH=(inputH-2*input_padding-kernel_size)//stride+1+2*output_padding\n",
    "    outputW=(inputW-2*input_padding-kernel_size)//stride+1+2*output_padding\n",
    "    ret = f\"{maxOrAvg}({input_padding},{INPUTARRAY},{D},{inputH},{inputW},{kernel_size},{stride},{output_padding},{OUTPUTARRAY},{PADDINGMASK});\\n\"\n",
    "    return {'inputH': outputH,'inputW': outputW,'fDefs': prevfDefs, 'fCalls': prevfCalls + ret + '\\n'}\n",
    "\n",
    "\n",
    "\n",
    "def genBitLinearBitPython(weight,bias_flag,BIASARRAYNAME,in_channels,out_channels,\n",
    "                        INPUTARRAYNAME,OUTPUTARRAYNAME, \n",
    "                        inputH,inputW,input_padding,PADDINGMASKNAME,\n",
    "                        absmeanweight,neg_slope,num_bits,non_linearity):\n",
    "\n",
    "    #how will we call our funcntions????\n",
    "    with open(\"curFNameNo.txt\",\"r\") as f:\n",
    "        num = int(f.read())\n",
    "    with open(\"curFNameNo.txt\",\"w\") as f:\n",
    "        f.write(str(num+1))\n",
    "    with open(\"layerNames.txt\", \"r\") as f:\n",
    "        allNames = f.read().split('\\n')\n",
    "        funcName = allNames[num]\n",
    "    #what will be the function's interface?\n",
    "    #layerName(I,O,ioutStart,ioutEndPlusOne,istart,jstart,stride)\n",
    "    fDefCode = f'''\n",
    "void {funcName}(float I[],float O[]){{\n",
    "'''\n",
    "    inputArrayLen = in_channels*(inputH-2*input_padding)*(inputW-2*input_padding)\n",
    "    for j in range(0,out_channels):\n",
    "        fDefCode+=f'O[{j}]+='#copy with no pad causes this confusing reversal\n",
    "        for it in range(0,inputArrayLen):\n",
    "            if weight[j][it] == 1:\n",
    "                fDefCode+=f'+I[{it}]'\n",
    "            if weight[j][it] == -1:\n",
    "                fDefCode+=f'-I[{it}]'\n",
    "        fDefCode+='+0;'\n",
    "    fDefCode+=f'}}'\n",
    "\n",
    "    fCallCode = f'''\n",
    "copyWithNoPad({INPUTARRAYNAME},{OUTPUTARRAYNAME},{PADDINGMASKNAME},{in_channels},{inputH},{inputW},{input_padding});\n",
    "absmax = layernormQuantizationBiasNextpadding({in_channels},{inputH-2*input_padding},{inputW-2*input_padding},{out_channels},1,{OUTPUTARRAYNAME},{PADDINGMASKNAME},{BIASARRAYNAME},{INPUTARRAYNAME},0,{bias_flag},1,{num_bits},true);\n",
    "st = clock();\n",
    "{funcName}({OUTPUTARRAYNAME},{INPUTARRAYNAME});\n",
    "end = clock();\n",
    "time_spent_in_conv += (double)(end - st);\n",
    "dequantRelu({INPUTARRAYNAME},{PADDINGMASKNAME},{out_channels*1*1},absmax,{absmeanweight},{num_bits},{neg_slope},{non_linearity});\n",
    "'''\n",
    "    return {'fDef': fDefCode, 'fCall': fCallCode}\n",
    "\n",
    "\n",
    "\n",
    "def genCodeDS(prevfDefs,prevfCalls,W1,AMW1,B1NAME,W2,AMW2,B2NAME,BFL,\n",
    "              in_channels,out_channels,\n",
    "       kernel_size,stride,groups,INPUTARRAYNAME,INTERARRAYNAME,OUTPUTARRAYNAME,\n",
    "       PADDINGMASKNAME,output_padding,inputH,inputW):\n",
    "    depth_code = genBitConvBitPythonNOIIB(weight=W1,bias_flag=BFL,\n",
    "                                          BIASARRAYNAME=B1NAME,\n",
    "                             in_channels=in_channels,\n",
    "                             out_channels=in_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             dilation=1,\n",
    "                             groups=in_channels,\n",
    "                             INPUTARRAYNAME=INPUTARRAYNAME,\n",
    "                             OUTPUTARRAYNAME=INTERARRAYNAME, \n",
    "                             inputH=inputH,inputW=inputW,\n",
    "                             output_padding=0,\n",
    "                             PADDINGMASKNAME=PADDINGMASKNAME,\n",
    "                             absmeanweight=AMW1,\n",
    "                             neg_slope=0.1,num_bits=8,\n",
    "                             non_linearity='false')#False\n",
    "    inputH_new = (inputH-kernel_size)//stride+1 \n",
    "    inputW_new = (inputW-kernel_size)//stride+1 \n",
    "    sep_code = genBitConvBitPythonNOIIB(weight=W2,bias_flag=BFL,\n",
    "                                        BIASARRAYNAME=B2NAME,\n",
    "                             in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=1,stride=1,\n",
    "                             dilation=1,groups=1,\n",
    "                             INPUTARRAYNAME=INTERARRAYNAME,\n",
    "                             OUTPUTARRAYNAME=OUTPUTARRAYNAME, \n",
    "                             inputH=inputH_new,inputW=inputW_new,\n",
    "                             output_padding=output_padding,\n",
    "                             PADDINGMASKNAME=PADDINGMASKNAME,\n",
    "                             absmeanweight=AMW2,\n",
    "                             neg_slope=0.1,num_bits=8,\n",
    "                             non_linearity='true')#True\n",
    "    outputH = (inputH_new-1)//1+1 + 2*output_padding\n",
    "    outputW = (inputW_new-1)//1+1 + 2*output_padding\n",
    "    return {'inputH':outputH,\n",
    "            'inputW':outputW,\n",
    "            'fDefs':prevfDefs+depth_code['fDef']+sep_code['fDef'],\n",
    "            'fCalls':prevfCalls+depth_code['fCall']+sep_code['fCall']}\n",
    "\n",
    "class CNNBLOCK_DSBitGenCode(nn.Module):#i should use more *ags and **kwargs\n",
    "    def __init__(self, weightAbsMeanD,weightAbsMeanS,in_channels_, out_channels_,\n",
    "                 kernel_size_, stride_, \n",
    "                 padding_, bias_,nextPadding,IAN,INTERAN,OAN,PMN,B1NAME,B2NAME):\n",
    "        super().__init__()\n",
    "        self.AMW1,self.AMW2 = weightAbsMeanD,weightAbsMeanS\n",
    "        self.BFL = bias_\n",
    "        biasAsVal = False\n",
    "        if bias_ == 'true':\n",
    "            biasAsVal = True\n",
    "        self.in_channels,self.out_channels = in_channels_,out_channels_\n",
    "        self.kernel_size,self.stride = kernel_size_,stride_\n",
    "        self.next_padding = nextPadding\n",
    "        self.INPUTARRAYNAME = IAN\n",
    "        self.INTERARRAYNAME = INTERAN\n",
    "        self.OUTPUTARRAYNAME = OAN\n",
    "        self.PADDINGMASKNAME = PMN\n",
    "        self.B1NAME = B1NAME\n",
    "        self.B2NAME = B2NAME\n",
    "        self.conv_depth = BitConv2dBitProba(weightAbsMean = weightAbsMeanD,in_channels = in_channels_, \n",
    "                                    out_channels =in_channels_, \n",
    "                                    kernel_size =kernel_size_, \n",
    "                                    stride = stride_, \n",
    "                                    padding = padding_, \n",
    "                                    dilation = 1,\n",
    "                                    groups =in_channels_,\n",
    "                                    bias = biasAsVal)\n",
    "        self.conv_sep = BitConv2dBitProba(weightAbsMean = weightAbsMeanS,in_channels = in_channels_, \n",
    "                                  out_channels = out_channels_, \n",
    "                                  kernel_size=1, \n",
    "                                  stride = 1,\n",
    "                                  padding= 0, \n",
    "                                  dilation=1,\n",
    "                                  groups = 1,\n",
    "                                  bias=False)\n",
    "        self.lrlu = nn.LeakyReLU(0.1)\n",
    "    def forward(self, x):\n",
    "        return genCodeDS(prevfDefs=x['fDefs'],\n",
    "                         prevfCalls=x['fCalls'],\n",
    "                  W1=self.conv_depth.weight,\n",
    "                  AMW1=self.AMW1,\n",
    "                  B1NAME=self.B1NAME,\n",
    "                  W2=self.conv_sep.weight,\n",
    "                  AMW2=self.AMW2,\n",
    "                  B2NAME=self.B2NAME,\n",
    "                  BFL=self.BFL,\n",
    "                  in_channels=self.in_channels,\n",
    "                  out_channels=self.out_channels,\n",
    "                  kernel_size=self.kernel_size,\n",
    "                  stride=self.stride,\n",
    "                  groups=self.in_channels,\n",
    "                  INPUTARRAYNAME=self.INPUTARRAYNAME,\n",
    "                  INTERARRAYNAME=self.INTERARRAYNAME,\n",
    "                  OUTPUTARRAYNAME=self.OUTPUTARRAYNAME,\n",
    "                  PADDINGMASKNAME=self.PADDINGMASKNAME,\n",
    "                  output_padding=self.next_padding,\n",
    "                  inputH=x['inputH'],inputW=x['inputW'])#or maybe biases should be external...\n",
    "\n",
    "class BitLinearBitGenCode(nn.Linear):\n",
    "    def __init__(self,bias_flag,BIASARRAYNAME,in_channels,out_channels,\n",
    "                        INPUTARRAYNAME,OUTPUTARRAYNAME, \n",
    "                        input_padding,PADDINGMASKNAME,\n",
    "                        absmeanweight,neg_slope,num_bits,non_linearity):\n",
    "        bias_as_val = False\n",
    "        if bias_flag == 'true':\n",
    "            bias_as_val = True\n",
    "        super().__init__(in_channels,out_channels,bias_as_val)\n",
    "        self.bias_flag = bias_flag\n",
    "        self.BIASARRAYNAME = BIASARRAYNAME\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.INPUTARRAYNAME = INPUTARRAYNAME\n",
    "        self.OUTPUTARRAYNAME = OUTPUTARRAYNAME\n",
    "        self.input_padding = input_padding\n",
    "        self.PADDINGMASKNAME = PADDINGMASKNAME\n",
    "        self.absmeanweight = absmeanweight\n",
    "        self.neg_slope = neg_slope\n",
    "        self.num_bits = num_bits\n",
    "        self.non_linearity = non_linearity\n",
    "    def forward(self,x):\n",
    "        resultDict = genBitLinearBitPython(weight=self.weight,\n",
    "                bias_flag=self.bias_flag,\n",
    "                BIASARRAYNAME=self.BIASARRAYNAME,\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                INPUTARRAYNAME=self.INPUTARRAYNAME,\n",
    "                OUTPUTARRAYNAME=self.OUTPUTARRAYNAME, \n",
    "                inputH=x['inputH'],\n",
    "                inputW=x['inputW'],\n",
    "                input_padding=self.input_padding,\n",
    "                PADDINGMASKNAME=self.PADDINGMASKNAME,\n",
    "                absmeanweight=self.absmeanweight,\n",
    "                neg_slope=self.neg_slope,\n",
    "                num_bits=self.num_bits,\n",
    "                non_linearity=self.non_linearity)\n",
    "        return {'inputH':1,'outputH':1,\n",
    "                'fCalls':x['fCalls']+resultDict['fCall'],\n",
    "                'fDefs':x['fDefs']+resultDict['fDef']}\n",
    "\n",
    "class RESBLOCKBitGenCode(nn.Module):\n",
    "    def __init__(self, ANTC,list_of_params):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        self.ARRAYNAMETOCOPY = ANTC\n",
    "        self.list_of_params = list_of_params\n",
    "        self.STARTINGARRAY = list_of_params[0][9]\n",
    "        self.NAMEOFARRAYTOSAVETO = list_of_params[0][9]\n",
    "        self.inputDim = list_of_params[0][2]\n",
    "        self.NAMEOFTHELASTARRAY = list_of_params[-1][11]\n",
    "        for block in self.list_of_params:\n",
    "            modules.append(CNNBLOCK_DSBitGenCode(weightAbsMeanD=block[0],\n",
    "                                          weightAbsMeanS=block[1],\n",
    "                                          in_channels_=block[2],\n",
    "                                          out_channels_=block[3],\n",
    "                                          kernel_size_=block[4], \n",
    "                                          stride_=block[5], \n",
    "                                          padding_=block[6], \n",
    "                                          bias_=block[7],\n",
    "                                          nextPadding=block[8],\n",
    "                                          IAN=block[9],\n",
    "                                          INTERAN=block[10],\n",
    "                                          OAN=block[11],\n",
    "                                          PMN=block[12],\n",
    "                                          B1NAME=block[13],\n",
    "                                          B2NAME=block[14]))\n",
    "        self.suka = nn.Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        inputH,inputW = x['inputH'],x['inputW']\n",
    "        prevfCalls = x['fCalls']\n",
    "        prevfDefs = x['fDefs']\n",
    "        #theFirstBit = prevfCalls+f\"\\nfor (i = 0; i < {self.inputDim*inputH*inputW}; i++){{\\n\\t{self.ARRAYNAMETOCOPY}[i] = {self.STARTINGARRAY}[i];}}\\n\"\n",
    "        theFirstBit = prevfCalls+f\"memcpy({self.ARRAYNAMETOCOPY}, {self.STARTINGARRAY}, {self.inputDim*inputH*inputW}*sizeof(float));\\n\"\n",
    "        almostThere = self.suka({'inputH':inputH,'inputW':inputW,'fCalls':'','fDefs':''})\n",
    "        #theLastBit = f\"\\nfor (i = 0; i < {self.inputDim*almostThere['inputH']*almostThere['inputW']}; i++){{\\n\\t{self.NAMEOFTHELASTARRAY}[i]+={self.ARRAYNAMETOCOPY}[i];}}\\n\"\n",
    "        theLastBit = f\"for (i = 0; i < 8*({self.inputDim*almostThere['inputH']*almostThere['inputW']}/8); i+=8){{\\n\"\n",
    "        theLastBit += f\"__m256 cur0 = _mm256_load_ps(&{self.NAMEOFTHELASTARRAY}[i]);\\n\"\n",
    "        theLastBit += f\"__m256 cur1 = _mm256_load_ps(&{self.ARRAYNAMETOCOPY}[i]);\\n\"\n",
    "        theLastBit += f\"cur0 = _mm256_add_ps(cur0, cur1);\\n\"\n",
    "        theLastBit += f\"_mm256_store_ps(&{self.NAMEOFTHELASTARRAY}[i], cur0);\\n}}\\n\"\n",
    "        theLastBit += f\"for (i = 8*({self.inputDim*almostThere['inputH']*almostThere['inputW']}/8); i < {self.inputDim*almostThere['inputH']*almostThere['inputW']}; i++){{\\n\"\n",
    "        theLastBit += f\"\\t{self.NAMEOFTHELASTARRAY}[i]+={self.ARRAYNAMETOCOPY}[i];}}\\n\"\n",
    "        return {'inputH':almostThere['inputH'],\n",
    "                'inputW':almostThere['inputW'],\n",
    "                'fCalls':theFirstBit+almostThere['fCalls']+theLastBit,\n",
    "                'fDefs':prevfDefs+almostThere['fDefs']}#and here we'll have the operation of addition...\n",
    "\n",
    "\n",
    "class YOLONETBitGenCode(nn.Module):#now let's modify it.....\n",
    "    def __init__(self, lowam,arrAName,arrBName,copyArrName,padArrName):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2,2)       \n",
    "        self.arrA = arrAName\n",
    "        self.arrB = arrBName\n",
    "        self.arrC = copyArrName\n",
    "        self.arrP = padArrName\n",
    "        self.convBlock1 = CNNBLOCK_DSBitGenCode(lowam[0],lowam[1],3, 64, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{0}',f'BIASES.b{1}')       \n",
    "        self.resblock64_1 = RESBLOCKBitGenCode(self.arrC,[[lowam[2],lowam[3],64, 64, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{2}',f'BIASES.b{3}'], [lowam[4],lowam[5],64,64, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{4}',f'BIASES.b{5}'] ])\n",
    "        self.resblock64_2 = RESBLOCKBitGenCode(self.arrC,[[lowam[6],lowam[7],64, 64, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{6}',f'BIASES.b{7}'], [lowam[8],lowam[9],64,64, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{8}',f'BIASES.b{9}'] ])\n",
    "        self.resblock64_3 = RESBLOCKBitGenCode(self.arrC,[[lowam[10],lowam[11],64, 64, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{10}',f'BIASES.b{11}'], [lowam[12],lowam[13],64,64, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{12}',f'BIASES.b{13}'] ])#????????????????????????????????????????????????????????????????????????????? What's the problem here???\n",
    "        #pool here A0 --> A1\n",
    "        self.resblock64_4 = RESBLOCKBitGenCode(self.arrC,[[lowam[14],lowam[15],64, 64, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{14}',f'BIASES.b{15}'], [lowam[16],lowam[17],64,64, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{16}',f'BIASES.b{17}'] ])\n",
    "        self.resblock64_5 = RESBLOCKBitGenCode(self.arrC,[[lowam[18],lowam[19],64, 64, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{18}',f'BIASES.b{19}'], [lowam[20],lowam[21],64,64, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{20}',f'BIASES.b{21}'] ])\n",
    "        #pool here A1 --> A0\n",
    "        self.convInter_64_128 = CNNBLOCK_DSBitGenCode(lowam[22],lowam[23],64, 128, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{22}',f'BIASES.b{23}')\n",
    "        self.resblock128_1 = RESBLOCKBitGenCode(self.arrC,[[lowam[24],lowam[25],128, 128, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{24}',f'BIASES.b{25}'], [lowam[26],lowam[27],128, 128, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{26}',f'BIASES.b{27}']])\n",
    "        self.resblock128_2 = RESBLOCKBitGenCode(self.arrC,[[lowam[28],lowam[29],128, 128, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{28}',f'BIASES.b{29}'], [lowam[30],lowam[31],128, 128, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{30}',f'BIASES.b{31}']])\n",
    "        #pool here A0 --> A1\n",
    "        self.resblock128_3 = RESBLOCKBitGenCode(self.arrC,[[lowam[32],lowam[33],128, 128, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{32}',f'BIASES.b{33}'], [lowam[34],lowam[35],128, 128, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{34}',f'BIASES.b{35}']])\n",
    "        #pool here A1 --> A0\n",
    "        self.resblock128_4 = RESBLOCKBitGenCode(self.arrC,[[lowam[36],lowam[37],128, 128, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{36}',f'BIASES.b{37}'], [lowam[38],lowam[39],128, 128, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{38}',f'BIASES.b{39}']])\n",
    "        self.resblock128_5 = RESBLOCKBitGenCode(self.arrC,[[lowam[40],lowam[41],128, 128, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{40}',f'BIASES.b{41}'], [lowam[42],lowam[43],128, 128, 3,1,1,'false',1,self.arrA,self.arrB,self.arrA,self.arrP,f'BIASES.b{42}',f'BIASES.b{43}']])\n",
    "        #pool here A0 --> A1\n",
    "        self.convInter_128_256 = CNNBLOCK_DSBitGenCode(lowam[44],lowam[45],128, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{44}',f'BIASES.b{45}')\n",
    "        self.resblock256_1 = RESBLOCKBitGenCode(self.arrC,[[lowam[46],lowam[47],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{46}',f'BIASES.b{47}'], [lowam[48],lowam[49],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{48}',f'BIASES.b{49}']])\n",
    "        self.resblock256_2 = RESBLOCKBitGenCode(self.arrC,[[lowam[50],lowam[51],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{50}',f'BIASES.b{51}'], [lowam[52],lowam[53],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{52}',f'BIASES.b{53}']])\n",
    "        self.resblock256_3 = RESBLOCKBitGenCode(self.arrC,[[lowam[54],lowam[55],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{54}',f'BIASES.b{55}'], [lowam[56],lowam[57],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{56}',f'BIASES.b{57}']])\n",
    "        self.resblock256_4 = RESBLOCKBitGenCode(self.arrC,[[lowam[58],lowam[59],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{58}',f'BIASES.b{59}'], [lowam[60],lowam[61],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{60}',f'BIASES.b{61}']])\n",
    "        self.resblock256_5 = RESBLOCKBitGenCode(self.arrC,[[lowam[62],lowam[63],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{62}',f'BIASES.b{63}'], [lowam[64],lowam[65],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{64}',f'BIASES.b{65}']])\n",
    "        self.resblock256_6 = RESBLOCKBitGenCode(self.arrC,[[lowam[66],lowam[67],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{66}',f'BIASES.b{67}'], [lowam[68],lowam[69],256, 256, 3,1,1,'false',1,self.arrB,self.arrA,self.arrB,self.arrP,f'BIASES.b{68}',f'BIASES.b{69}']])\n",
    "        self.avgPool = nn.AvgPool2d(2,3)\n",
    "        self.fc = BitLinearBitGenCode('true',f'BIASES.b{70}',256,10,\n",
    "                        self.arrB,self.arrA, \n",
    "                        1,self.arrP,\n",
    "                        lowam[70],0.1,8,'false')#(lowam[70],256, 10)\n",
    "        #print('aaaaa suka', self.fc.weight)\n",
    "\n",
    "    def forward(self,x):\n",
    "        print('started generating code')\n",
    "        x = self.convBlock1(x)\n",
    "        x = self.resblock64_1(x)\n",
    "        x = self.resblock64_2(x)\n",
    "        x = self.resblock64_3(x)\n",
    "        x = genPoolCodePython(input_padding=1,maxOrAvg='maxpool2d',prevfDefs = x['fDefs'],prevfCalls = x['fCalls'],INPUTARRAY=self.arrA,D=64,inputH=x['inputH'],inputW=x['inputW'],kernel_size=2,stride=2,output_padding=1,OUTPUTARRAY=self.arrB,PADDINGMASK=self.arrP)\n",
    "        x = self.resblock64_4(x)\n",
    "        x = self.resblock64_5(x)\n",
    "        x = genPoolCodePython(input_padding=1,maxOrAvg='maxpool2d',prevfDefs = x['fDefs'],prevfCalls = x['fCalls'],INPUTARRAY=self.arrB,D=64,inputH=x['inputH'],inputW=x['inputW'],kernel_size=2,stride=2,output_padding=1,OUTPUTARRAY=self.arrA,PADDINGMASK=self.arrP)\n",
    "        x = self.convInter_64_128(x)\n",
    "        x = self.resblock128_1(x)\n",
    "        x = self.resblock128_2(x)\n",
    "        x = genPoolCodePython(input_padding=1,maxOrAvg='maxpool2d',prevfDefs = x['fDefs'],prevfCalls = x['fCalls'],INPUTARRAY=self.arrA,D=128,inputH=x['inputH'],inputW=x['inputW'],kernel_size=2,stride=2,output_padding=1,OUTPUTARRAY=self.arrB,PADDINGMASK=self.arrP)\n",
    "        x = self.resblock128_3(x)\n",
    "        x = genPoolCodePython(input_padding=1,maxOrAvg='maxpool2d',prevfDefs = x['fDefs'],prevfCalls = x['fCalls'],INPUTARRAY=self.arrB,D=128,inputH=x['inputH'],inputW=x['inputW'],kernel_size=2,stride=2,output_padding=1,OUTPUTARRAY=self.arrA,PADDINGMASK=self.arrP)\n",
    "        x = self.resblock128_4(x)\n",
    "        x = self.resblock128_5(x)\n",
    "        x = genPoolCodePython(input_padding=1,maxOrAvg='maxpool2d',prevfDefs = x['fDefs'],prevfCalls = x['fCalls'],INPUTARRAY=self.arrA,D=128,inputH=x['inputH'],inputW=x['inputW'],kernel_size=2,stride=2,output_padding=1,OUTPUTARRAY=self.arrB,PADDINGMASK=self.arrP)\n",
    "        x = self.convInter_128_256(x)\n",
    "        x = self.resblock256_1(x)\n",
    "        x = self.resblock256_2(x)\n",
    "        x = self.resblock256_3(x)\n",
    "        x = self.resblock256_4(x)\n",
    "        x = self.resblock256_5(x)\n",
    "        x = self.resblock256_6(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1e3d9b-87e5-48ea-bea9-45b1e0981465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorCMP(t1,t2,eps):\n",
    "    return torch.all(torch.abs(t1-t2) < eps).item()\n",
    "def tensorMDP(tcorrect,t2):\n",
    "    absDif = torch.abs(tcorrect-t2)\n",
    "    meandif = torch.mean(absDif).item()\n",
    "    meanabsval = torch.mean(torch.abs(tcorrect)).item()\n",
    "    percent = 100*meandif/(meanabsval+1e-7)\n",
    "    themax = torch.max(absDif)\n",
    "    return {'mean%':percent,\n",
    "            'max%':100*themax.item()/meanabsval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918bb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081627 273862\n"
     ]
    }
   ],
   "source": [
    "primeNet = YOLONET()\n",
    "primeNet.load_state_dict(torch.load('theCIFARbitnetWithNoBatchNorm (1).pt',\n",
    "                                   map_location=DEVICE))\n",
    "\n",
    "\n",
    "nzero = 0\n",
    "ntotal = 0\n",
    "for layer, item in primeNet.state_dict().items():\n",
    "    if layer.split('.')[-1] != 'bias':\n",
    "        mean_abs_val = item.abs().mean().item()\n",
    "        eps = 1e-7\n",
    "        scaled_weights= item / (mean_abs_val + eps)\n",
    "        bin_weights_no_grad = torch.clamp(torch.round(scaled_weights), min=-1, max=1)\n",
    "        bin_weights_with_grad = (bin_weights_no_grad - item).detach() + item\n",
    "        for iter in bin_weights_with_grad.flatten():\n",
    "            if iter.item() == 0:\n",
    "                nzero += 1\n",
    "            ntotal += 1\n",
    "print(ntotal, nzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "814bd3ad-998c-482f-bbfa-fbf4e1d5caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started generating code\n",
      "gen is done\n",
      "\u001b[01m\u001b[Kresult_code_old.c:\u001b[m\u001b[K In function \u001b[01m\u001b[Kmain\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[Kresult_code_old.c:11768:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat \u001b[01m\u001b[K%d\u001b[m\u001b[K expects argument of type \u001b[01m\u001b[Kint *\u001b[m\u001b[K, but argument 3 has type \u001b[01m\u001b[K_Bool *\u001b[m\u001b[K [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "11768 |     fscanf(fileMask,\"\u001b[01;35m\u001b[K%d\u001b[m\u001b[K\", \u001b[32m\u001b[K&P[k]\u001b[m\u001b[K);\n",
      "      |                      \u001b[01;35m\u001b[K~^\u001b[m\u001b[K   \u001b[32m\u001b[K~~~~~\u001b[m\u001b[K\n",
      "      |                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K   \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
      "      |                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K   \u001b[32m\u001b[K_Bool *\u001b[m\u001b[K\n",
      "      |                       \u001b[01;35m\u001b[Kint *\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kresult_code_old.c:11767:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of \u001b[01m\u001b[Kfscanf\u001b[m\u001b[K declared with attribute \u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "11767 |     \u001b[01;35m\u001b[Kfscanf(fileArr,\"%f\", &A[k])\u001b[m\u001b[K;\n",
      "      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kresult_code_old.c:11768:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of \u001b[01m\u001b[Kfscanf\u001b[m\u001b[K declared with attribute \u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "11768 |     \u001b[01;35m\u001b[Kfscanf(fileMask,\"%d\", &P[k])\u001b[m\u001b[K;\n",
      "      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "compilation done\n",
      "all done\n",
      "OUTPUTLEN =  10\n",
      "REAL OUTPUT LEN =  10\n",
      "{'mean%': 0.20351535997927506, 'max%': 0.3693874058819628}\n",
      "CPU time spent: 0.052109\n",
      "Time spent in conv: 0.049278\n",
      "Time spent in conv: 94.567157 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's recall how do we actually quantize weights....\n",
    "if True:\n",
    "    \n",
    "    primeNet = YOLONET()\n",
    "    primeNet.load_state_dict(torch.load('theCIFARbitnetWithNoBatchNorm (1).pt',\n",
    "                                   map_location=DEVICE))\n",
    "    BIASES = []\n",
    "    lowam = []\n",
    "    qWeights = OrderedDict()\n",
    "    for layer,item in primeNet.state_dict().items():\n",
    "        if layer.split('.')[-1] == 'bias':\n",
    "            BIASES.pop(-1)\n",
    "            BIASES.append(item)\n",
    "            qWeights[layer] = item\n",
    "        else:\n",
    "            BIASES.append(0)#what to do with this silly problem introduced completely by myself??\n",
    "            mean_abs_val = item.abs().mean().item()\n",
    "            eps = 1e-7\n",
    "            scaled_weights= item / (mean_abs_val + eps)\n",
    "            bin_weights_no_grad = torch.clamp(torch.round(scaled_weights), min=-1, max=1)\n",
    "            bin_weights_with_grad = (bin_weights_no_grad - item).detach() + item\n",
    "            qWeights[layer] = bin_weights_with_grad\n",
    "            lowam.append(mean_abs_val)\n",
    "    #probaNet = YOLONETBitProba(lowam)\n",
    "    #probaNet.load_state_dict(qWeights)\n",
    "    genTextNet = YOLONETBitGenCode(lowam,'A','B','C','P')\n",
    "    genTextNet.load_state_dict(qWeights)\n",
    "\n",
    "    for i in range(0, len(BIASES)):\n",
    "        if torch.is_tensor(BIASES[i]):\n",
    "            temp = []\n",
    "            for j in range(0,len(BIASES[i])):\n",
    "                temp.append(BIASES[i][j].item())\n",
    "            BIASES[i] = temp\n",
    "    with open(\"layerNames.txt\",\"w\") as f:\n",
    "        f.write('')\n",
    "    for layer in genTextNet.state_dict().keys():\n",
    "        if layer.split('.')[-1] == 'weight':\n",
    "            with open(\"layerNames.txt\",\"a\") as f:\n",
    "                f.write(layer.replace('.','_')+'\\n')\n",
    "    with open(\"curFNameNo.txt\",'w') as f:\n",
    "        f.write(str(0))\n",
    "\n",
    "    D,Hnp,Wnp = 3,32,32\n",
    "    pad = 1\n",
    "    H,W = Hnp+2*pad,Wnp+2*pad\n",
    "    inputTensor = torch.rand((D,Hnp,Wnp))\n",
    "    outputReal = torch.squeeze(primeNet(torch.unsqueeze(inputTensor,0)),0)\n",
    "    if ALLLAYERS == False:\n",
    "        outputRealPadded = torch.zeros((outputReal.shape[0], outputReal.shape[1]+2*pad, outputReal.shape[2]+2*pad))\n",
    "        outputRealPadded[:,pad:pad+outputReal.shape[1],pad:pad+outputReal.shape[2]] = outputReal\n",
    "        outputReal = outputRealPadded.clone()\n",
    "    outputRealFlat = outputReal.flatten()\n",
    "    with open(\"outputReal.txt\", \"w\") as f:\n",
    "        for i in range(0, len(outputRealFlat)):\n",
    "            f.write(str(outputRealFlat[i].item())+'\\n')\n",
    "    #outputProba = probaNet(inputTensor)\n",
    "    maxLen = 64*(32+2*pad)*(32+2*pad)#100_000#65536\n",
    "    inputTensorPadded = torch.zeros((D,H,W))\n",
    "    inputTensorPadded[:,pad:pad+Hnp,pad:pad+Wnp] = inputTensor.clone()\n",
    "    At = torch.zeros(maxLen,dtype=torch.float32)\n",
    "    At[0:D*H*W] = inputTensorPadded.flatten().clone()\n",
    "    Bt = torch.zeros(maxLen,dtype=torch.float32)\n",
    "    Ct = torch.zeros(maxLen,dtype=torch.float32)\n",
    "    Pt = torch.ones(maxLen,dtype=torch.float32)\n",
    "    padM = torch.zeros((D,H,W))\n",
    "    padM[:,pad:pad+Hnp,pad:pad+Wnp] = torch.ones((D,Hnp,Wnp))\n",
    "    Pt[0:D*H*W] = padM.flatten()\n",
    "    A,B,C,P =[0]*maxLen,[0]*maxLen,[0]*maxLen,[1]*maxLen\n",
    "    for i in range(0,maxLen):\n",
    "        A[i],B[i],C[i],P[i] = At[i].item(),Bt[i].item(),Ct[i].item(),Pt[i].item()\n",
    "    with open('inputTensorPaddedFlat.txt','w') as f:\n",
    "        for i in range(0, D*H*W):\n",
    "            f.write(str(A[i])+'\\n')\n",
    "    with open('paddingmask.txt','w') as f:\n",
    "        for i in range(0, D*H*W):\n",
    "            f.write(str(int(P[i]))+'\\n')\n",
    "    zah = zah0\n",
    "    parts = zah.split(\"/*\\nINSERT YOUR BIAS DECLARATION CODE HERE\\n*/\")\n",
    "    bias_struct = genBiasStructDeclaration(BIASES)\n",
    "    zah = parts[0] + bias_struct + parts[1]\n",
    "    parts = zah.split(\"/*\\nINSERT YOU BIAS INIT HERE\\n*/\")\n",
    "    bias_filled = genPopulateBiasStruct(BIASES)\n",
    "    zah = parts[0] + bias_filled + parts[1]\n",
    "\n",
    "    outputMine = genTextNet({'inputH':34,'inputW':34,'fDefs':'','fCalls':''})\n",
    "    print('gen is done')\n",
    "\n",
    "    if os.path.exists('result_code_old.c'):\n",
    "        os.remove('result_code_old.c')\n",
    "    if os.path.exists('result_code_old.dll'):\n",
    "        os.remove('result_code_old.dll')\n",
    "    if os.path.exists('log.txt'):\n",
    "        os.remove('log.txt')\n",
    "    if os.path.exists('output.txt'):\n",
    "        os.remove('output.txt')\n",
    "    \n",
    "    parts = zah.split(\"/*\\nINSERT YOUR FUNCTION DEFINITIONS HERE\\n*/\")\n",
    "    zah = parts[0] + outputMine['fDefs'] + parts[1]\n",
    "    parts = zah.split(\"/*\\nINSERT YOUR FUNCTION CALLS HERE\\n*/\")\n",
    "    zah = parts[0] + outputMine['fCalls'] + parts[1]\n",
    "    with open('result_code_old.c', 'w') as f:\n",
    "        f.write(zah)\n",
    "\n",
    "    #!gcc -march=x86-64-v3 -fPIC -shared -O2 -o C:/Users/oleks/Desktop1/AI_CODE_C/result_code.dll C:/Users/oleks/Desktop1/AI_CODE_C/result_code.c\n",
    "    !cd /home/oleksiy-tsuber/AI_CODE_C\n",
    "    !gcc -march=native -O3 -o result_code_old result_code_old.c -lm\n",
    "    #!valgrind --tool=callgrind --dump-instr=yes --collect-jumps=yes --simulate-cache=yes ./result_code_old\n",
    "    #-g flag makes so that debug info is present in the executable\n",
    "    print('compilation done')\n",
    "    if True:\n",
    "        !./result_code_old\n",
    "        print('all done')\n",
    "        #clib = ctypes.CDLL(\"C:/Users/oleks/Desktop1/AI_CODE_C/result_code.dll\")\n",
    "        #clib.main()\n",
    "        output = torch.as_tensor(np.loadtxt(\"output.txt\"), dtype=torch.float32)\n",
    "        print(\"OUTPUTLEN = \", len(output))\n",
    "        print(\"REAL OUTPUT LEN = \", len(outputReal.flatten()))\n",
    "        print(tensorMDP(outputReal.flatten(),output))\n",
    "        with open(\"log.txt\") as f:\n",
    "            print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446de8b5-944f-4122-9406-d4ac9f3a9122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on real net:  0.0302274227142334\n"
     ]
    }
   ],
   "source": [
    "inp = torch.rand((1,3,32,32))\n",
    "st = time.time()\n",
    "temp = primeNet(inp)\n",
    "end = time.time()\n",
    "print('time spent on real net: ', end - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978114a6-c1d9-46c1-9dc0-814fece0b92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur iter =  1\n",
      "cur iter =  2\n",
      "cur iter =  3\n",
      "cur iter =  4\n",
      "cur iter =  5\n",
      "cur iter =  6\n",
      "cur iter =  7\n",
      "cur iter =  8\n",
      "cur iter =  9\n",
      "cur iter =  10\n",
      "cur iter =  11\n",
      "cur iter =  12\n",
      "cur iter =  13\n",
      "cur iter =  14\n",
      "cur iter =  15\n",
      "cur iter =  16\n",
      "cur iter =  17\n",
      "cur iter =  18\n",
      "cur iter =  19\n",
      "cur iter =  20\n",
      "cur iter =  21\n",
      "cur iter =  22\n",
      "cur iter =  23\n",
      "cur iter =  24\n",
      "cur iter =  25\n",
      "cur iter =  26\n",
      "cur iter =  27\n",
      "cur iter =  28\n",
      "cur iter =  29\n",
      "cur iter =  30\n",
      "cur iter =  31\n",
      "cur iter =  32\n",
      "cur iter =  33\n",
      "cur iter =  34\n",
      "cur iter =  35\n",
      "cur iter =  36\n",
      "cur iter =  37\n",
      "cur iter =  38\n",
      "cur iter =  39\n",
      "cur iter =  40\n",
      "cur iter =  41\n",
      "cur iter =  42\n",
      "cur iter =  43\n",
      "cur iter =  44\n",
      "cur iter =  45\n",
      "cur iter =  46\n",
      "cur iter =  47\n",
      "cur iter =  48\n",
      "cur iter =  49\n",
      "cur iter =  50\n",
      "cur iter =  51\n",
      "cur iter =  52\n",
      "cur iter =  53\n",
      "cur iter =  54\n",
      "cur iter =  55\n",
      "cur iter =  56\n",
      "cur iter =  57\n",
      "cur iter =  58\n",
      "cur iter =  59\n",
      "cur iter =  60\n",
      "cur iter =  61\n",
      "cur iter =  62\n",
      "cur iter =  63\n",
      "cur iter =  64\n",
      "cur iter =  65\n",
      "cur iter =  66\n",
      "cur iter =  67\n",
      "cur iter =  68\n",
      "cur iter =  69\n",
      "cur iter =  70\n",
      "cur iter =  71\n",
      "cur iter =  72\n",
      "cur iter =  73\n",
      "cur iter =  74\n",
      "cur iter =  75\n",
      "cur iter =  76\n",
      "cur iter =  77\n",
      "cur iter =  78\n",
      "cur iter =  79\n",
      "cur iter =  80\n",
      "cur iter =  81\n",
      "cur iter =  82\n",
      "cur iter =  83\n",
      "cur iter =  84\n",
      "cur iter =  85\n",
      "cur iter =  86\n",
      "cur iter =  87\n",
      "cur iter =  88\n",
      "cur iter =  89\n",
      "cur iter =  90\n",
      "cur iter =  91\n",
      "cur iter =  92\n",
      "cur iter =  93\n",
      "cur iter =  94\n",
      "cur iter =  95\n",
      "cur iter =  96\n",
      "cur iter =  97\n",
      "cur iter =  98\n",
      "cur iter =  99\n",
      "cur iter =  100\n",
      "On avg we spent 94.54939357999996 % time in conv\n",
      "Avg time for C bitnet: 0.052473519999999996\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    n = 100\n",
    "    sum0,sum1,sum2 = 0,0,0\n",
    "    for i in range(0, n):\n",
    "        if (i+1) % 1 == 0:\n",
    "            print('cur iter = ', i+1)\n",
    "        #clib.main()\n",
    "        !cd /home/oleksiy-tsuber/AI_CODE_C\n",
    "        !./result_code_old\n",
    "        with open(\"log.txt\") as f:\n",
    "            text = f.read()\n",
    "            parts = text.split('Time spent in conv: ')\n",
    "            theBitWeCareAbout0 = parts[-1]\n",
    "            theBitWeCareAbout0 = float(theBitWeCareAbout0.split(' %')[0])\n",
    "            sum0 += theBitWeCareAbout0\n",
    "            theBitWeCareAbout1 = parts[0]\n",
    "            theBitWeCareAbout1 = float(theBitWeCareAbout1.split('CPU time spent: ')[1])\n",
    "            sum1 += theBitWeCareAbout1\n",
    "            \n",
    "    s0,s1,s2 = sum0/n,sum1/n,sum2/n\n",
    "    print('On avg we spent', s0, '% time in conv')\n",
    "    print(\"Avg time for C bitnet:\", s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe047826-40a3-414c-9a43-8ae40eaae68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    !valgrind --tool=callgrind --dump-instr=yes --collect-jumps=yes --simulate-cache=yes ./result_code_old"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
